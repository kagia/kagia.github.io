---
layout: post
author: kagia
tags: ['machine learning']
---

A couple of days ago I set my self the challenge of **learning machine-learning in just 4 weeks!**. If you haven't already, read my first post [here]({{ site.baseurl }}{% post_url 2018-12-29-learning-machine-learning %}). So how did the first day go?

In this post, I briefly discuss the challenges I faced, what I learned and what changes I'm making going forward.

Blocking out my ideal one and a half hours of study, proved to be difficult. I decided to spend this new-years with my mother and time with her came first. That said, I still managed to get my hour and a half by breaking it up into smaller half-hour blocks throughout the day.

The Google course is pretty rapid at presenting you with new concepts and terms. So I'd highly suggest to anyone else coming up with their own study plan to factor in some extra time to re-watch a course video every now and then. However, I still feel very confident that I'm on track to complete the course on time. Let's go over what I learnt.

At the center of machine learning is a *model*, the brain if you will. The *model* is used to *infer* or *predict* something about the information it is fed. For example, a *model* can be created that given the contents of an email, classifies that email as spam or not spam.

That said, the beginning of the course doesn't really dwell on how *models* work, but rather how they are trained or "learned".  To "learn" a model you first feed it with *labelled examples* . This is quite similar to how children are taught the names of animals by being shown lablled pictures. Once trained the model can now be fed with an unlabeled example and in turn infer the correct label for it. In fact, some models can go even further and predict labels for examples they have never seen before as would be the case with a spam detection model.

I also learnt that the training process is far from perfect and not an exact science. It turns out that every model has certain parameters that will affect how well and how fast it can learn. This is the part of the course I really found interesting. Training a model takes on nudging process where the parameters are ever so tweaked and the results measured. This happens over and over until satisfactory performance is gained. The exact details involve some light algebra theory but it's nothing that should worry anyone who has done high school maths.

**Note** I should also add that although I've talked only of models that produce discreet predefined labels, there are those that can produce continuous values. For example, a model that predicts the likelihood of a team winning a game.

So that's a brief overview of what I have learnt so far. Going forward I'll make a permanent change to my study plan to break up the study-time block in two. It not only gives me greater flexibility on a busy day but also allows me to maintain my focus throughout.

The next chapter in the course is titled "First steps with Tensor Flow", and if I'm not wrong that means code! I've enjoyed the theory but as I wrote in the first post; I learn best when I'm also applying the new knowledge. I can't wait to get started!

So there it is folks my first day. If you have advice, questions or comments you can reach me on twitter [@benjaminkagia](https://twitter.com/BenjaminKagia).

I'll try to post at least twice a week on my progress and learnings.

Grow happiness!